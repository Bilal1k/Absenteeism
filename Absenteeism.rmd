---
title: "Absenteeism"
author: "Bilal Khaiar"
date: "12/17/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

This database was created with records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil.

An attempt to biuld a machine learning algorithm that can predict hours of absenteeism will be made. Mutiple algorithms will be tried and an enseble will be built from the best models.

The dataset was aquired from UCI Machine Learning Repository. For more information click ([here](https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work)).

# Methods

## Packages

The following packages are needed to run this project's code.
```{r libraries, results='hide', message=FALSE, warning=FALSE}
pack <- c("knitr", "tidyverse", "textreadr", "lubridate",
          "stringr", "scales","rpart","randomForest","Rborist",
          "caret", "arm", "mboost","MASS",
          "glmnet","Matrix","ipred","e1071", "nnet",
          "deepnet")

# Check if all packages are already installed, if not, install them.
new.packages <- pack[!(pack %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

# loop to load all packages.
lapply(pack, library, character.only = TRUE)
```

## Preparing the Data

The data set and the attached info file will both be downloaded.

```{r download, message=FALSE, warning=FALSE, results='hide'}
temp <- tempfile()
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00445/Absenteeism_at_work_AAA.zip",temp)
data <- read.csv2(unz(temp, "Absenteeism_at_work.csv"))
unlink(temp)

temp <- tempfile()
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00445/Absenteeism_at_work_AAA.zip",temp)
info <- read_docx(unzip(temp, "Attribute Information.docx"), skip = 1)[1:44]
unlink(temp)
```


## Exploratory Analysis
In order to get insights into the dataset, some exploratory analysis will be made.

```{r explore}
str(data)
sum(apply(data, 2, anyNA))
length(unique(data$Absenteeism.time.in.hours))
summary(as.factor(data$Absenteeism.time.in.hours))
sort(unique(data$Reason.for.absence))
```

The dataset has 21 variables and 751 observations. most of observations are stored as integers although some are actually catagorical.

There are no missing values which will make anaylsis and prediction easier.

Although with only 19 unique values, it would have been possible to  treat the target variable as an ordinal catagorical variable rather than a continuous one, but with multiple bins having only one or few observations, it would be hard to treat this as a classifying problem.

No absenteeism due to reason 20.

A list with naems of reasons will be created from the info file.

```{r reasons}
reason_n <- c(0,info[4:22],info[24]) # get reasons names form info file. Skip the 23rd line as reason 20 is absent form the data
c <- c("patient follow-up", "medical consultation",
       "blood donation", "laboratory examination",
       "unjustified absence", "physiotherapy",
       "dental consultation") # list non ICD reasons
reason_n[22:28] <- c # add non ICD reasons
```


## Visualizaiton
An attempt to visualize some aspect of the data will be made in order the better understand the distribution and the relationship between some variables. 

```{r distribution, fig.show='hold', out.width='25%'}
data %>% ggplot(aes(Day.of.the.week)) + geom_histogram()
data %>% ggplot(aes(ID)) + geom_histogram()
data %>% ggplot(aes(Service.time)) + geom_histogram()
data %>% ggplot(aes(Absenteeism.time.in.hours)) + geom_histogram()
```
\
Most of the data is not normally distributed.

```{r reasons distribution}
data %>% ggplot(aes(as.factor(Reason.for.absence))) + 
  geom_histogram(stat = 'count') +
  scale_x_discrete(label = str_trunc(reason_n, 40)) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1, size = 10))
```

Most common reason for absence is medical consultations, while

In order to get a quick glimps into how each variable causes variations in the target, a loop that will calculate the standard deviation for the target variable when other variable are grouped into bins will be made.

```{r target sd loop}
variable <- colnames(data)[1:20] # is a `vector` now
data1 <- list() # initialize as a `list`
for(i in variable){ 
  f <- data %>%
    group_by_at(i) %>% #changed to `group_by_at`
    summarise(median = median(Absenteeism.time.in.hours))
  data1[[i]] <- sd(f$median)
}

kable(arrange(tibble(var = names(data1),
                     absent.sd = unlist(data1)),
              desc(absent.sd)))
```

A couple of error bars will be made to check how some variables can affect the target.

````{r error bars}
data %>% group_by(Day.of.the.week) %>%
  summarise(n = n(), 
            avg = mean(Absenteeism.time.in.hours), 
            sd = sd(Absenteeism.time.in.hours),
            se = sd/sqrt(n)) %>%
  ggplot(aes(as.factor(Day.of.the.week), avg)) + geom_point() +
  geom_errorbar(aes(ymin=avg - 2*se, ymax=avg+2*se)) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1))

data %>% group_by(Work.load.Average.day) %>%
  summarise(n = n(), 
            avg = mean(Absenteeism.time.in.hours), 
            sd = sd(Absenteeism.time.in.hours),
            se = sd/sqrt(n)) %>%
  ggplot(aes(as.factor(Work.load.Average.day), avg)) + geom_point() +
  geom_errorbar(aes(ymin=avg - 2*se, ymax=avg+2*se)) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1)) 

data %>% group_by(Reason.for.absence) %>%
  summarise(n = n(), 
            avg = mean(Absenteeism.time.in.hours), 
            sd = sd(Absenteeism.time.in.hours),
            se = sd/sqrt(n)) %>%
  filter(se <= 3.5) %>%
  ggplot(aes(as.factor(Reason.for.absence), avg)) + geom_point() +
  scale_x_discrete(label = str_trunc(reason_n, 40)) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1, size = 10)) +
  geom_errorbar(aes(ymin=avg - 2*se, ymax=avg+2*se))
```

 Most of the avareges' standard errors overlap. this shows that if each variable is used on its own, it will not be a good predicter in a liner model.  


# Wrangling
catogorical data wil be changed into class factor and the  work load avarages will be converted into a numerical vector.
```{r change class}
data_w <- data %>% 
  mutate_at(c(1,2,3,4,5), as.factor) %>% 
  mutate(Work.load.Average.day =
            as.numeric(levels(Work.load.Average.day))[Work.load.Average.day])
```

An index for numeric variables will be created in order ease analysis of numeric features.
```{r numeric index}
nums <- unlist(lapply(data_w, is.numeric))
```

# Preprocessing

## Outliers
A box plot will be created for all numerical variable to visualize outliers.

```{outliers boxplot}
data_w %>% gather(key = "key", value ="value", -c(names(data_w[,!nums]))) %>% 
  ggplot(aes(key,value)) + geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 65, hjust = 1, size = 10))
```
\

variabels that had visible outliers in the previous plot will be examined

```{r outliers exam}
head(sort(data_w$Age, decreasing = TRUE))
head(sort(data_w$Height, decreasing = TRUE))
head(sort(data_w$Height))
summary(data$Height)
summary(data_w$Hit.target)
summary(data_w$Service.time)
summary(data_w$Absenteeism.time.in.hours)
```

OUtliers can decrease model accuracy. A funciton that can process the outliers will be created. it will be called caps_outliers(). 

for the porpose of this project, outliers will be definded according to Tukey Fences.

Outliers are defined as values falling below Q1-1.5(Q3-Q1) or above Q3+1.5(Q3-Q1) where Q1 and Q3 represent the 1st and the 3rd quartiles respectivly. for more information. For more information, click([here](https://archive.org/details/exploratorydataa00tuke_0))

Outliers will be exchanged with values from the 5th percentile and the 95th percentile.

Any changes to the target variable should be done with care. Depending on the purpose of the project, this can artificially increase accuracy because the target will not represent real world data any more. We will basically be predicting a new version of the data that is trimmed and relativly easy to predict.


```{r cap outliers}
caps_outliers <- function(x){
  qnt <- quantile(x, c(0.25, 0.75))
  cap <- quantile(x, c(.05, .95 ) )
  H <- 1.5 * IQR(x)
  x[x < (qnt[1] - H)] <- cap[1]
  x[x > (qnt[2] + H)] <- cap[2]
  x
}

# create a new numerical variables index that doesn't include the target
nums_t <- nums
nums_t[21] <- FALSE

# use the new function to convert outliers from all numerical variables exceptTarget 
data_w[,nums_t] <- apply(data_w[,nums_t],2,caps_outliers)
```

## Scaling

Numerical variables will be scaled to avoid model bias towards variables with longer ranges. all variable will be represented with values from 0 to 1.

```{r scaling}
data_w[,nums] <- apply(data_w[,nums], 2, scales::rescale)
```

## Multicollinearity

Multicollinearity is high correlation between features. It can reduce prediction accuracy.

A Correlation matrix will be created and visualized as a heatmap
```{r corr matrix}
cor.dat <- cor(data_w[,nums])
heatmap(cor.dat, scale="column")

with(data_w,c(cor(Weight, Body.mass.index)))
```

Since weight and body mass index are highly correlated, the body mass index variable will be removed to avoid collinearity. Intuitvly, it is calculated from weight and height and as both are avaible in the data, it was removed.

```{r remove BMI}
data_w$Body.mass.index <- NULL
```

## Encoding catogorical variable

Many machine learning algorithms do not support categorical values. All catogorical variables will binarized using one hot encoding technique. Each catagorical variable will be reprisented with a number of new variables equal to its number of levels. each variable will have values of ones and zeros to represent the presence or absece of each specific level from each observation.

```{r binarizaiton}
data_w <- dummy_cols(data_w, remove_selected_columns = TRUE)

dim(data_w)
```

due to binarization, the data dimentions increased from 20 predictors to 100. High dimentional data can result in low preforming machine learning algorithms.

## Principal Component Analysis (PCA)

PCA is a dimention reduction technique that transform the data into a set of values of linearly uncorrelated variables called principal components. The first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. It can be used to reduce dimention without loosing data variablity.

The target variable will be seperated from the data before tranformation.

```{r pca}
data_w$Absenteeism.time.in.hours -> y
data_w$Absenteeism.time.in.hours <- NULL
pca <-  prcomp(data_w)
dim(pca$x)


# Proportion of variance explained
variablity <- pca$sdev^2/sum(pca$sdev^2)
sum(variablity[1:45])
```

by using the first 45 components we keep about 94% of the Variablity
data_pca <- pca$x[,1:45]

# Building Models

## Partitioning the data
The data and the target will  be split into a test and a train part. the test set will be 20% of the data.

```{r split, warning=FALSE}
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = data_w$Absenteeism.time.in.hours,
                                  times = 1, p = 0.2, list = FALSE)
train <- data_w[-test_index,]
test <- data_w[test_index,]
train_y <- y[-test_index]
test_y <- y[test_index]
```


## models
a loop will try different models using the caret train  funciton.

```{r models, results='hide', message=FALSE, warning=FALSE}
models <- c("lm", "mlp", "knn", "penalized","rqnc","krlsPoly","foba","BstLm")

# train models on dataset
fits <- lapply(models, function(model){
  set.seed(2, sample.kind = "Rounding")
  print(model)
  caret::train(train,train_y, method = model)
})
names(fits) <- models
```

predict target using each model on test set data.

```{r prediction, results='hide', message=FALSE, warning=FALSE }
# predict using models
y_hat <- sapply(fits, function(fits){
  predict(fits, test)
})
```


# Results

The predition accuracy will be checked in this section.

The accuracy of each model using Root Mean Square Error (RMSE) will be assessed

```{r rmse, message=FALSE, warning=FALSE  }
acc <- apply(y_hat, 2, function(x){
  RMSE(y_hats_rescaled,y)
})
acc
```

A simple linear regression model preforms better than all other models. The type and size of the data plays a factor on which model to use.

Those RMSEs are difficult to interpret due the scaled nature of the tareget. It will be resclaed to the original range by reversing the min max scaling formula.

```{r rescaling}

real_y <- data$Absenteeism.time.in.hours

acc_res <- acc*(max(real_y)-min(real_y))-min(real_y)
summary(acc_res)
acc_res
```


y_hats_rescaled <- y_hat*(max(y)-min(y))-min(y)


## Ensembles 

An ensemble is a product of predictions from different models. an esemble that take the avarge results of the best 4 models will be built.

```{r ensemble}
ensemble <- apply(y_hat[,c(1,2,6,7)], 1, mean)


acc_ens <- RMSE(ensemble, test_y)
acc_ens

acc_ens*(max(real_y)-min(real_y))-min(real_y)

# individual methods comparision to the ensemble
acc > mean(ensemble == mnist_27$test$y)

# mean of training set accuracy estimates
train_acc <- lapply(fits, function(x){
  tacc <- x$results$Accuracy
  mean(tacc)
})
mean(train_acc[[1]])


# Ref
Tukey, John W (1977). Exploratory Data Analysis. OCLC 3058187.
https://archive.org/details/exploratorydataa00tuke_0